{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1QCWinW0d6Tpq7mzxKQv-roX8A8Lqq0_8",
      "authorship_tag": "ABX9TyN1OkCeQZMjzFBYG6nh4J1m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carokhan/apriltags/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import math\n",
        "import cv2\n",
        "from keras.utils import Sequence\n",
        "import tensorflow as tf\n",
        "from tqdm.keras import TqdmCallback\n",
        "import os\n",
        "import datetime\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "\n",
        "outs = ['center_score', 'center_link_score',\n",
        "       'ordered_corner_0_x', 'ordered_corner_0_y', 'ordered_corner_1_x',\n",
        "       'ordered_corner_1_y', 'ordered_corner_2_x', 'ordered_corner_2_y',\n",
        "       'ordered_corner_3_x', 'ordered_corner_3_y', 'corner_anchor_0_x',\n",
        "       'corner_anchor_0_y', 'corner_anchor_1_x', 'corner_anchor_1_y',\n",
        "       'corner_anchor_2_x', 'corner_anchor_2_y', 'corner_anchor_3_x',\n",
        "       'corner_anchor_3_y', 'corner_score_0', 'corner_score_1',\n",
        "       'corner_score_2', 'corner_score_3', 'center_pos_x', 'center_pos_y']\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size=16):\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = [[row[y] for y in outs] for (idx, row) in df.iterrows()]\n",
        "        self.im_list = self.df['image'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(math.ceil(len(self.df) / float(self.batch_size)))\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        return self.labels[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "\n",
        "    def get_batch_features(self, idx):\n",
        "        return [cv2.imread(im) for im in self.im_list[idx * self.batch_size: (1 + idx) * self.batch_size]]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "          x_features = self.get_batch_features(idx)\n",
        "          y_labels = self.get_batch_labels(idx)\n",
        "          if type(x_features) == type(None) or type(y_labels) == type(None):\n",
        "            print(idx)\n",
        "          batch_x = np.array(x_features)\n",
        "          batch_y = np.array(y_labels)\n",
        "          with open(\"generator.log\", \"w\") as f:\n",
        "            f.write(str({\"x_images\": [im for im in self.im_list[idx * self.batch_size: (1 + idx) * self.batch_size]], \"x\": batch_x, \"y\": batch_y}))\n",
        "            f.write(\"\\n\")\n",
        "          return batch_x, batch_y\n",
        "\n",
        "class MergeMetrics(Callback):\n",
        "\n",
        "    def __init__(self,**kargs):\n",
        "        super(MergeMetrics,self).__init__(**kargs)\n",
        "\n",
        "    def on_train_batch_end(self, epoch, logs={}):\n",
        "        logs[\"merged_acc\"] = logs[\"center_link_score_accuracy\"] *0.5 + logs[\"corner_score_0_accuracy\"] *0.125 + logs[\"corner_score_1_accuracy\"]*0.125 +  logs[\"corner_score_2_accuracy\"]*0.125 +  logs[\"corner_score_3_accuracy\"]*0.125\n",
        "        logs[\"merged_mse\"] = logs[\"ordered_corner_1_x_mse\"] + logs[\"ordered_corner_1_y_mse\"] + logs[\"ordered_corner_2_x_mse\"] + logs[\"ordered_corner_2_y_mse\"] + logs[\"ordered_corner_3_x_mse\"] + logs[\"ordered_corner_3_y_mse\"] + logs[\"corner_anchor_0_x_mse\"] + logs[\"corner_anchor_0_y_mse\"] + logs[\"corner_anchor_1_x_mse\"] + logs[\"corner_anchor_1_y_mse\"] + logs[\"corner_anchor_2_x_mse\"] + logs[\"corner_anchor_2_y_mse\"] + logs[\"corner_anchor_3_x_mse\"] + logs[\"corner_anchor_3_y_mse\"] + logs[\"center_pos_x_mse\"] + logs[\"center_pos_y_mse\"]\n",
        "\n",
        "    def on_test_batch_end(self, epoch, logs={}):\n",
        "        logs[\"merged_acc\"] = logs[\"center_link_score_accuracy\"] *0.5 + logs[\"corner_score_0_accuracy\"] *0.125 + logs[\"corner_score_1_accuracy\"]*0.125 +  logs[\"corner_score_2_accuracy\"]*0.125 +  logs[\"corner_score_3_accuracy\"]*0.125\n",
        "        logs[\"merged_mse\"] = logs[\"ordered_corner_1_x_mse\"] + logs[\"ordered_corner_1_y_mse\"] + logs[\"ordered_corner_2_x_mse\"] + logs[\"ordered_corner_2_y_mse\"] + logs[\"ordered_corner_3_x_mse\"] + logs[\"ordered_corner_3_y_mse\"] + logs[\"corner_anchor_0_x_mse\"] + logs[\"corner_anchor_0_y_mse\"] + logs[\"corner_anchor_1_x_mse\"] + logs[\"corner_anchor_1_y_mse\"] + logs[\"corner_anchor_2_x_mse\"] + logs[\"corner_anchor_2_y_mse\"] + logs[\"corner_anchor_3_x_mse\"] + logs[\"corner_anchor_3_y_mse\"] + logs[\"center_pos_x_mse\"] + logs[\"center_pos_y_mse\"]\n",
        "\n",
        "    def on_epoch_begin(self,epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        logs[\"merged_acc\"] = logs[\"center_link_score_accuracy\"] *0.5 + logs[\"corner_score_0_accuracy\"] *0.125 + logs[\"corner_score_1_accuracy\"]*0.125 +  logs[\"corner_score_2_accuracy\"]*0.125 +  logs[\"corner_score_3_accuracy\"]*0.125\n",
        "        logs[\"merged_mse\"] = logs[\"ordered_corner_1_x_mse\"] + logs[\"ordered_corner_1_y_mse\"] + logs[\"ordered_corner_2_x_mse\"] + logs[\"ordered_corner_2_y_mse\"] + logs[\"ordered_corner_3_x_mse\"] + logs[\"ordered_corner_3_y_mse\"] + logs[\"corner_anchor_0_x_mse\"] + logs[\"corner_anchor_0_y_mse\"] + logs[\"corner_anchor_1_x_mse\"] + logs[\"corner_anchor_1_y_mse\"] + logs[\"corner_anchor_2_x_mse\"] + logs[\"corner_anchor_2_y_mse\"] + logs[\"corner_anchor_3_x_mse\"] + logs[\"corner_anchor_3_y_mse\"] + logs[\"center_pos_x_mse\"] + logs[\"center_pos_y_mse\"]\n",
        "\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\n",
        "# # This is the TPU initialization code that has to be at the beginning.\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "# strategy = tf.distribute.TPUStrategy(resolver)\n"
      ],
      "metadata": {
        "id": "-FdxF4ud8FTZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://85df-72-82-59-111.ngrok.io/final_data.zip -O ../../final_data.zip"
      ],
      "metadata": {
        "id": "HKFsy9weAmqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e8307b-e841-4574-ef30-322a98d9ffc5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-19 06:30:09--  https://85df-72-82-59-111.ngrok.io/final_data.zip\n",
            "Resolving 85df-72-82-59-111.ngrok.io (85df-72-82-59-111.ngrok.io)... 3.17.7.232, 2600:1f16:d83:1200::6e:3\n",
            "Connecting to 85df-72-82-59-111.ngrok.io (85df-72-82-59-111.ngrok.io)|3.17.7.232|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20040157140 (19G) [application/zip]\n",
            "Saving to: ‘../../final_data.zip’\n",
            "\n",
            "../../final_data.zi 100%[===================>]  18.66G  6.87MB/s    in 46m 58s \n",
            "\n",
            "2022-10-19 07:17:09 (6.78 MB/s) - ‘../../final_data.zip’ saved [20040157140/20040157140]\n",
            "\n",
            "unzip:  cannot find or open /content/final_data.zip, /content/final_data.zip.zip or /content/final_data.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /final_data.zip -d /content/final_zip"
      ],
      "metadata": {
        "id": "nsowmuQiIi5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /final_data.zip\n",
        "path = \"/content/final_zip/final_data/\"\n",
        "df = pd.read_csv(os.path.join(path, \"colab_data.csv\"))\n",
        "shuffled = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "training_set = shuffled[:8000]\n",
        "validation_set = shuffled[8000:10000]\n",
        "test_set = shuffled[10000:]"
      ],
      "metadata": {
        "id": "dwV4KyPVuBB2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(os.path.join(path, \"colab_data.csv\"))\n",
        "df[\"image\"] = df[\"image\"].apply(lambda x: os.path.join(path ,\"/\".join(x.split(\"/\")[-2:])))\n",
        "df.head()\n",
        "\n",
        "shuffled = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "training_set = shuffled[:8000]\n",
        "validation_set = shuffled[8000:10000]\n",
        "test_set = shuffled[10000:]"
      ],
      "metadata": {
        "id": "VcX5oPGrr3Xd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = []\n",
        "losses = []\n",
        "loss_wts = []\n",
        "metrics = {}\n",
        "# with strategy.scope():\n",
        "mnet = MobileNet()\n",
        "mnet.trainable = False\n",
        "mnet_out = mnet.layers[-4].output\n",
        "for name in outs:\n",
        "    activaton = 'softmax' #'softmax' if 'score' in name else 'linear'\n",
        "    loss = 'mse' #'binary_crossentropy' if 'score' in name else 'mse'\n",
        "    metric =  'accuracy' if 'score' in name else 'mse'\n",
        "    metrics[name] = metric\n",
        "    losses.append(loss)\n",
        "    if name != \"center_link_score\":\n",
        "        loss_wts.append(0.5)\n",
        "    else:\n",
        "        loss_wts.append(1)\n",
        "    t = Dense(49, activation='relu', name='pre_'+name)(mnet_out)\n",
        "    outputs.append(Dense(1, activation=activaton, name=name)(t))\n",
        "model = Model(inputs=mnet.input, outputs=outputs)\n",
        "# model.compile(loss=losses, optimizer=\"adam\")\n",
        "model.compile(loss=losses, optimizer=\"adam\", metrics=metrics, loss_weights=loss_wts)"
      ],
      "metadata": {
        "id": "8FttwI02DPr6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = MergeMetrics()\n",
        "\n",
        "callbacks = [\n",
        "            tf.keras.callbacks.ModelCheckpoint(\"apriltag_model\", monitor=\"center_link_score_accuracy\", mode=\"max\", save_best_only=True),\n",
        "            checkpoint\n",
        "            ]\n",
        "train_gen = DataGenerator(training_set, batch_size=32)\n",
        "validate_gen = DataGenerator(validation_set, batch_size=32)"
      ],
      "metadata": {
        "id": "Zc_1j-j-DXCd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x=train_gen, epochs=3, verbose=1, validation_data=validate_gen, steps_per_epoch=250, use_multiprocessing=False, callbacks=callbacks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKpcMRATDbaI",
        "outputId": "b450d2b9-4fb3-4f76-cd05-8decf8a4dcfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dh_eaWaQJl48"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}