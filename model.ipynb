{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1QCWinW0d6Tpq7mzxKQv-roX8A8Lqq0_8",
      "authorship_tag": "ABX9TyMwQfPw1+SbCzm5bB+Df+4l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carokhan/apriltags/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "import os \n",
        "\n",
        "os.chdir(\"/content/drive/My Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hv6Wv6aAXww",
        "outputId": "ba3c73b2-efeb-4e5b-b368-890d89e35871"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import math\n",
        "import cv2\n",
        "from keras.utils import Sequence\n",
        "import tensorflow as tf\n",
        "from tqdm.keras import TqdmCallback\n",
        "import os\n",
        "import datetime\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "outs = ['center_score', 'center_link_score',\n",
        "       'ordered_corner_0_x', 'ordered_corner_0_y', 'ordered_corner_1_x',\n",
        "       'ordered_corner_1_y', 'ordered_corner_2_x', 'ordered_corner_2_y',\n",
        "       'ordered_corner_3_x', 'ordered_corner_3_y', 'corner_anchor_0_x',\n",
        "       'corner_anchor_0_y', 'corner_anchor_1_x', 'corner_anchor_1_y',\n",
        "       'corner_anchor_2_x', 'corner_anchor_2_y', 'corner_anchor_3_x',\n",
        "       'corner_anchor_3_y', 'corner_score_0', 'corner_score_1',\n",
        "       'corner_score_2', 'corner_score_3', 'center_pos_x', 'center_pos_y']\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size=16):\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = [[row[y] for y in outs] for (idx, row) in df.iterrows()]\n",
        "        self.im_list = self.df['image'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(math.ceil(len(self.df) / float(self.batch_size)))\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        return self.labels[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "\n",
        "    def get_batch_features(self, idx):\n",
        "        return [cv2.imread(im) for im in self.im_list[idx * self.batch_size: (1 + idx) * self.batch_size]]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.get_batch_features(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "        return np.array(batch_x), np.array(batch_y)\n",
        "\n",
        "class MergeMetrics(Callback):\n",
        "\n",
        "    def __init__(self,**kargs):\n",
        "        super(MergeMetrics,self).__init__(**kargs)\n",
        "\n",
        "    def on_train_batch_end(self, epoch, logs={}):\n",
        "        logs[\"merged_acc\"] = logs[\"center_link_score_accuracy\"] *0.5 + logs[\"corner_score_0_accuracy\"] *0.125 + logs[\"corner_score_1_accuracy\"]*0.125 +  logs[\"corner_score_2_accuracy\"]*0.125 +  logs[\"corner_score_3_accuracy\"]*0.125\n",
        "        logs[\"merged_mse\"] = logs[\"ordered_corner_1_x_mse\"] + logs[\"ordered_corner_1_y_mse\"] + logs[\"ordered_corner_2_x_mse\"] + logs[\"ordered_corner_2_y_mse\"] + logs[\"ordered_corner_3_x_mse\"] + logs[\"ordered_corner_3_y_mse\"] + logs[\"corner_anchor_0_x_mse\"] + logs[\"corner_anchor_0_y_mse\"] + logs[\"corner_anchor_1_x_mse\"] + logs[\"corner_anchor_1_y_mse\"] + logs[\"corner_anchor_2_x_mse\"] + logs[\"corner_anchor_2_y_mse\"] + logs[\"corner_anchor_3_x_mse\"] + logs[\"corner_anchor_3_y_mse\"] + logs[\"center_pos_x_mse\"] + logs[\"center_pos_y_mse\"]\n",
        "\n",
        "    def on_test_batch_end(self, epoch, logs={}):\n",
        "        logs[\"merged_acc\"] = logs[\"center_link_score_accuracy\"] *0.5 + logs[\"corner_score_0_accuracy\"] *0.125 + logs[\"corner_score_1_accuracy\"]*0.125 +  logs[\"corner_score_2_accuracy\"]*0.125 +  logs[\"corner_score_3_accuracy\"]*0.125\n",
        "        logs[\"merged_mse\"] = logs[\"ordered_corner_1_x_mse\"] + logs[\"ordered_corner_1_y_mse\"] + logs[\"ordered_corner_2_x_mse\"] + logs[\"ordered_corner_2_y_mse\"] + logs[\"ordered_corner_3_x_mse\"] + logs[\"ordered_corner_3_y_mse\"] + logs[\"corner_anchor_0_x_mse\"] + logs[\"corner_anchor_0_y_mse\"] + logs[\"corner_anchor_1_x_mse\"] + logs[\"corner_anchor_1_y_mse\"] + logs[\"corner_anchor_2_x_mse\"] + logs[\"corner_anchor_2_y_mse\"] + logs[\"corner_anchor_3_x_mse\"] + logs[\"corner_anchor_3_y_mse\"] + logs[\"center_pos_x_mse\"] + logs[\"center_pos_y_mse\"]\n",
        "\n",
        "    def on_epoch_begin(self,epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        logs[\"merged_acc\"] = logs[\"center_link_score_accuracy\"] *0.5 + logs[\"corner_score_0_accuracy\"] *0.125 + logs[\"corner_score_1_accuracy\"]*0.125 +  logs[\"corner_score_2_accuracy\"]*0.125 +  logs[\"corner_score_3_accuracy\"]*0.125\n",
        "        logs[\"merged_mse\"] = logs[\"ordered_corner_1_x_mse\"] + logs[\"ordered_corner_1_y_mse\"] + logs[\"ordered_corner_2_x_mse\"] + logs[\"ordered_corner_2_y_mse\"] + logs[\"ordered_corner_3_x_mse\"] + logs[\"ordered_corner_3_y_mse\"] + logs[\"corner_anchor_0_x_mse\"] + logs[\"corner_anchor_0_y_mse\"] + logs[\"corner_anchor_1_x_mse\"] + logs[\"corner_anchor_1_y_mse\"] + logs[\"corner_anchor_2_x_mse\"] + logs[\"corner_anchor_2_y_mse\"] + logs[\"corner_anchor_3_x_mse\"] + logs[\"corner_anchor_3_y_mse\"] + logs[\"center_pos_x_mse\"] + logs[\"center_pos_y_mse\"]\n",
        "\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "# print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "# try:\n",
        "#   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "# except ValueError:\n",
        "#   raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "# from tensorflow.python.profiler import profiler_client\n",
        "\n",
        "# tpu_profile_service_address = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466')\n",
        "# print(profiler_client.monitor(tpu_profile_service_address, 100, 2))\n",
        "# tf.config.experimental_connect_to_cluster(tpu)\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "# tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "-FdxF4ud8FTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec1db743-6689-4a0d-b92e-f4b3b83caab6"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.9.2\n",
            "Running on TPU  ['10.42.150.226:8470']\n",
            "  Timestamp: 05:01:18\n",
            "  TPU type: TPU v2\n",
            "  Utilization of TPU Matrix Units (higher is better): 0.000%\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.42.150.226:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://85df-72-82-59-111.ngrok.io/final_data.zip -O ../../final_data.zip\n",
        "!unzip /content/final_data.zip -d /content/final_zip\n",
        "\n",
        "path = \"/content/final_zip/final_data/\"\n",
        "df = pd.read_csv(os.path.join(path, \"colab_data.csv\"))\n",
        "shuffled = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "training_set = shuffled[:8000]\n",
        "validation_set = shuffled[8000:10000]\n",
        "test_set = shuffled[10000:]"
      ],
      "metadata": {
        "id": "HKFsy9weAmqs"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnet = MobileNet()\n",
        "mnet.trainable = False\n",
        "mnet_out = mnet.layers[-4].output"
      ],
      "metadata": {
        "id": "qqqaKZHXDEZ3"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = []\n",
        "losses = []\n",
        "loss_wts = []\n",
        "metrics = {}\n",
        "for name in outs:\n",
        "    activaton = 'softmax' #'softmax' if 'score' in name else 'linear'\n",
        "    loss = 'mse' #'binary_crossentropy' if 'score' in name else 'mse'\n",
        "    metric =  'accuracy' if 'score' in name else 'mse'\n",
        "    metrics[name] = metric\n",
        "    losses.append(loss)\n",
        "    if name != \"center_link_score\":\n",
        "        loss_wts.append(0.5)\n",
        "    else:\n",
        "        loss_wts.append(1)\n",
        "    t = Dense(49, activation='relu', name='pre_'+name)(mnet_out)\n",
        "    outputs.append(Dense(1, activation=activaton, name=name)(t))\n",
        "model = Model(inputs=mnet.input, outputs=outputs)\n",
        "# model.compile(loss=losses, optimizer=\"adam\")\n",
        "model.compile(loss=losses, optimizer=\"adam\", metrics=metrics, loss_weights=loss_wts)"
      ],
      "metadata": {
        "id": "8FttwI02DPr6"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = MergeMetrics()\n",
        "\n",
        "callbacks = [\n",
        "            tf.keras.callbacks.ModelCheckpoint(\"apriltag_model\", monitor=\"center_link_score_accuracy\", mode=\"max\", save_best_only=True),\n",
        "            checkpoint\n",
        "            ]\n",
        "train_gen = DataGenerator(training_set, batch_size=64)\n",
        "validate_gen = DataGenerator(validation_set, batch_size=64)"
      ],
      "metadata": {
        "id": "Zc_1j-j-DXCd"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x=train_gen, epochs=3, verbose=1, validation_data=validate_gen, steps_per_epoch=125, use_multiprocessing=False, callbacks=callbacks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "PKpcMRATDbaI",
        "outputId": "3ce6cdb7-911f-4c93-9e52-5580e856446a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-8cb4ca98c045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: '__inference_train_function_102154' is neither a type of a primitive operation nor a name of a function registered in binary running on n-d1a32286-w-0. Make sure the operation or function is registered in the binary running in this process."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWLs17fWMavz",
        "outputId": "ef3e489b-a52f-4d8e-dff2-9cfd888ddc6c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-19 03:03:44--  https://85df-72-82-59-111.ngrok.io/final_data.zip\n",
            "Resolving 85df-72-82-59-111.ngrok.io (85df-72-82-59-111.ngrok.io)... 3.134.125.175, 2600:1f16:d83:1201::6e:1\n",
            "Connecting to 85df-72-82-59-111.ngrok.io (85df-72-82-59-111.ngrok.io)|3.134.125.175|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20040157140 (19G) [application/zip]\n",
            "Saving to: ‘../../final_data.zip’\n",
            "\n",
            "../../final_data.zi 100%[===================>]  18.66G  5.90MB/s    in 50m 35s \n",
            "\n",
            "2022-10-19 03:54:20 (6.30 MB/s) - ‘../../final_data.zip’ saved [20040157140/20040157140]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aAlVobgXMjG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nj9G5D7hjrtl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}